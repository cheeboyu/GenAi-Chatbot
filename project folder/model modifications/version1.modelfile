# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM qwen3:8b

FROM C:\Users\htsocadmin\.ollama\models\blobs\sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f
TEMPLATE """{{- if .Messages }}
{{- if or .System .Tools }}<|im_start|>system
{{- if .System }}
{{ .System }}
{{- end }}
{{- if .Tools }}

# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{{- range .Tools }}
{"type": "function", "function": {{ .Function }}}
{{- end }}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>
{{- end }}<|im_end|>
{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 -}}
{{- if eq .Role "user" }}<|im_start|>user
{{ .Content }}<|im_end|>
{{ else if eq .Role "assistant" }}<|im_start|>assistant
{{ if .Content }}{{ .Content }}
{{- else if .ToolCalls }}<tool_call>
{{ range .ToolCalls }}{"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}}
{{ end }}</tool_call>
{{- end }}{{ if not $last }}<|im_end|>
{{ end }}
{{- else if eq .Role "tool" }}<|im_start|>user
<tool_response>
{{ .Content }}
</tool_response><|im_end|>
{{ end }}
{{- if and (ne .Role "assistant") $last }}<|im_start|>assistant
{{ end }}
{{- end }}
{{- else }}
{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}"""
PARAMETER repeat_penalty 1
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>

SYSTEM You are a document-based Q&A assistant. Your capabilities are tools, completion, document analysis, document explaination, anything that requires you to answer the user as accurat Answer the user's question using only the provided document excerpts. Prioritize *accuracy, quality, and detail* above all else; speed is secondary. Carefully review *all* provided excerpts and synthesize the relevant information to form a complete answer. Do not rely on just one excerpt if multiple are relevant. If the answer is not in the provided text, state that clearly.
		
PARAMETER temperature 0.3
#Quality & Factuality: A low temperature reduces randomness, making the output more deterministic, focused, and adherent to the information in the retrieved context. Start lower (0.3) and increase slightly if the output feels too rigid.
PARAMETER num_ctx 4096
#Comprehensiveness & RAG: Crucial for RAG. Needs to be large enough to hold the user prompt plus multiple retrieved document chunks. 2048 is often too small. Increase based on your expected context size and hardware capability.
PARAMETER top_k 40
#Quality & Avoid Nonsense: Limits sampling to the top K likely tokens. Keeps the output grounded without being overly restrictive. The default 40 is reasonable, maybe slightly lower (30) combined with low temperature.
PARAMETER top_p 0.9
#Diversity from Context: Works with top_k. Keeping this relatively high allows the model to consider a wider range of likely tokens (whose cumulative probability is 0.9). With a low temperature already ensuring focus, this helps the model draw from different parts of the varied context provided by RAG, preventing hyperfocus.
PARAMETER mirostat_tau 4.0
#N/A if mirostat is 0. If using Mirostat, a slightly lower value (e.g., 4.0) could increase coherence, but start with the default (5.0).
PARAMETER seed	1
#Reproducibility (Optional): Set a specific seed if you need reproducible outputs for testing and evaluation. For production use where slight variations are acceptable, you can leave it at 0.